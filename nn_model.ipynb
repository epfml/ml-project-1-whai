{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from implementations import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the neural network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize the weights and biases for the input and hidden layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = 0.3  # Learning rate\n",
    "        self.lr_decay = 0.95  # Learning rate decay factor\n",
    "        self.min_lr = 0.001 \n",
    "\n",
    "        self.weights_input_hidden = np.random.rand(self.input_size, self.hidden_size)\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "\n",
    "        # Initialize the weights and biases for the hidden and output layers\n",
    "        self.weights_hidden_output = np.random.rand(self.hidden_size, self.output_size)\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward propagation\n",
    "        self.hidden_layer_input = np.dot(x, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_layer_output = sigmoid(self.hidden_layer_input)\n",
    "\n",
    "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.output_layer_output = sigmoid(self.output_layer_input)\n",
    "\n",
    "        return self.output_layer_output\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        # Backpropagation with cross-entropy loss\n",
    "        output = self.forward(x)\n",
    "        error = y - output\n",
    "\n",
    "        # Calculate gradients\n",
    "        delta_output = error\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += self.hidden_layer_output.T.dot(delta_output) * self.lr\n",
    "        delta_hidden = delta_output.dot(self.weights_hidden_output.T) * self.hidden_layer_output * (1 - self.hidden_layer_output)\n",
    "        self.weights_input_hidden += x.T.reshape(-1, 1).dot(delta_hidden) * self.lr\n",
    "        self.bias_output += np.sum(delta_output, axis=0, keepdims=True) * self.lr\n",
    "        self.bias_hidden += np.sum(delta_hidden, axis=0, keepdims=True) * self.lr\n",
    "\n",
    "    def train(self, X, y, epochs,batching=True, batch_size=16):\n",
    "        show_loss_every=100 if batching else 1\n",
    "        show_f1_every=1000 if batching else 1\n",
    "        prev_loss = float('inf')  # Store previous loss to check for stagnation\n",
    "        consecutive_bad_epochs = 0\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            indices = np.arange(len(X))\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            if batching:\n",
    "                batch_indices = indices[:batch_size]\n",
    "            else:\n",
    "                batch_indices=indices\n",
    "            x_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "\n",
    "            for j in range(len(x_batch)):\n",
    "                x = x_batch[j]\n",
    "                target = y_batch[j]\n",
    "                self.backward(x, target)\n",
    "            \n",
    "            \n",
    "            loss = self.calculate_cross_entropy_loss(X, y)\n",
    "            if epoch % show_loss_every == 0:\n",
    "                \n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "                print(f\"learning rate: {self.lr}\")\n",
    "\n",
    "            if epoch % show_f1_every == 0:\n",
    "                f1 = self.compute_f1_score(X, y)\n",
    "                print(f\"Epoch {epoch}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "            # Decay the learning rate\n",
    "            if loss > prev_loss:\n",
    "                consecutive_bad_epochs += 1\n",
    "                if consecutive_bad_epochs >= 3:\n",
    "                    self.lr *= self.lr_decay  # Decrease the learning rate\n",
    "            else:\n",
    "                consecutive_bad_epochs = 0\n",
    "\n",
    "            prev_loss = loss\n",
    "\n",
    "            # Ensure the learning rate doesn't go below a minimum value\n",
    "            if self.lr < self.min_lr:\n",
    "                self.lr = self.min_lr\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.round(self.forward(x))\n",
    "\n",
    "    def calculate_cross_entropy_loss(self, X, y):\n",
    "        predictions = self.forward(X)\n",
    "        # Avoid division by zero and numerical instability\n",
    "        epsilon = 1e-15\n",
    "        predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "        loss = - (y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def compute_f1_score(self, X, y):\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        false_negatives = 0\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            x = X[i]\n",
    "            target = y[i]\n",
    "            prediction = self.predict(x)\n",
    "            predictions.append(prediction)  # Store predictions\n",
    "\n",
    "            if target == 1 and prediction == 1:\n",
    "                true_positives += 1\n",
    "            elif target == 0 and prediction == 1:\n",
    "                false_positives += 1\n",
    "            elif target == 1 and prediction == 0:\n",
    "                false_negatives += 1\n",
    "        if true_positives==0 and (false_positives==0 or false_negatives==0):\n",
    "            return 0\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "        return f1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = pd.read_csv('data/x_train.csv')\n",
    "y_tr = pd.read_csv('data/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(x_tr.columns)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_STATE',\n",
       " 'FMONTH',\n",
       " 'IDATE',\n",
       " 'IMONTH',\n",
       " 'IDAY',\n",
       " 'IYEAR',\n",
       " 'DISPCODE',\n",
       " 'SEQNO',\n",
       " '_PSU',\n",
       " 'CTELENUM',\n",
       " 'PVTRESD1',\n",
       " 'COLGHOUS',\n",
       " 'STATERES',\n",
       " 'CELLFON3',\n",
       " 'LADULT',\n",
       " 'NUMADULT',\n",
       " 'NUMMEN',\n",
       " 'NUMWOMEN',\n",
       " 'CTELNUM1',\n",
       " 'CELLFON2',\n",
       " 'CADULT',\n",
       " 'PVTRESD2',\n",
       " 'CCLGHOUS',\n",
       " 'CSTATE',\n",
       " 'LANDLINE',\n",
       " 'HHADULT',\n",
       " 'GENHLTH',\n",
       " 'PHYSHLTH',\n",
       " 'MENTHLTH',\n",
       " 'POORHLTH',\n",
       " 'HLTHPLN1',\n",
       " 'PERSDOC2',\n",
       " 'MEDCOST',\n",
       " 'CHECKUP1',\n",
       " 'BPHIGH4',\n",
       " 'BPMEDS',\n",
       " 'BLOODCHO',\n",
       " 'CHOLCHK',\n",
       " 'TOLDHI2',\n",
       " 'CVDSTRK3',\n",
       " 'ASTHMA3',\n",
       " 'ASTHNOW',\n",
       " 'CHCSCNCR',\n",
       " 'CHCOCNCR',\n",
       " 'CHCCOPD1',\n",
       " 'HAVARTH3',\n",
       " 'ADDEPEV2',\n",
       " 'CHCKIDNY',\n",
       " 'DIABETE3',\n",
       " 'DIABAGE2',\n",
       " 'SEX',\n",
       " 'MARITAL',\n",
       " 'EDUCA',\n",
       " 'RENTHOM1',\n",
       " 'NUMHHOL2',\n",
       " 'NUMPHON2',\n",
       " 'CPDEMO1',\n",
       " 'VETERAN3',\n",
       " 'EMPLOY1',\n",
       " 'CHILDREN',\n",
       " 'INCOME2',\n",
       " 'INTERNET',\n",
       " 'WEIGHT2',\n",
       " 'HEIGHT3',\n",
       " 'PREGNANT',\n",
       " 'QLACTLM2',\n",
       " 'USEEQUIP',\n",
       " 'BLIND',\n",
       " 'DECIDE',\n",
       " 'DIFFWALK',\n",
       " 'DIFFDRES',\n",
       " 'DIFFALON',\n",
       " 'SMOKE100',\n",
       " 'SMOKDAY2',\n",
       " 'STOPSMK2',\n",
       " 'LASTSMK2',\n",
       " 'USENOW3',\n",
       " 'ALCDAY5',\n",
       " 'AVEDRNK2',\n",
       " 'DRNK3GE5',\n",
       " 'MAXDRNKS',\n",
       " 'FRUITJU1',\n",
       " 'FRUIT1',\n",
       " 'FVBEANS',\n",
       " 'FVGREEN',\n",
       " 'FVORANG',\n",
       " 'VEGETAB1',\n",
       " 'EXERANY2',\n",
       " 'EXRACT11',\n",
       " 'EXEROFT1',\n",
       " 'EXERHMM1',\n",
       " 'EXRACT21',\n",
       " 'EXEROFT2',\n",
       " 'EXERHMM2',\n",
       " 'STRENGTH',\n",
       " 'LMTJOIN3',\n",
       " 'ARTHDIS2',\n",
       " 'ARTHSOCL',\n",
       " 'JOINPAIN',\n",
       " 'SEATBELT',\n",
       " 'FLUSHOT6',\n",
       " 'FLSHTMY2',\n",
       " 'IMFVPLAC',\n",
       " 'PNEUVAC3',\n",
       " 'HIVTST6',\n",
       " 'HIVTSTD3',\n",
       " 'WHRTST10',\n",
       " 'PDIABTST',\n",
       " 'PREDIAB1',\n",
       " 'INSULIN',\n",
       " 'BLDSUGAR',\n",
       " 'FEETCHK2',\n",
       " 'DOCTDIAB',\n",
       " 'CHKHEMO3',\n",
       " 'FEETCHK',\n",
       " 'EYEEXAM',\n",
       " 'DIABEYE',\n",
       " 'DIABEDU',\n",
       " 'CAREGIV1',\n",
       " 'CRGVREL1',\n",
       " 'CRGVLNG1',\n",
       " 'CRGVHRS1',\n",
       " 'CRGVPRB1',\n",
       " 'CRGVPERS',\n",
       " 'CRGVHOUS',\n",
       " 'CRGVMST2',\n",
       " 'CRGVEXPT',\n",
       " 'VIDFCLT2',\n",
       " 'VIREDIF3',\n",
       " 'VIPRFVS2',\n",
       " 'VINOCRE2',\n",
       " 'VIEYEXM2',\n",
       " 'VIINSUR2',\n",
       " 'VICTRCT4',\n",
       " 'VIGLUMA2',\n",
       " 'VIMACDG2',\n",
       " 'CIMEMLOS',\n",
       " 'CDHOUSE',\n",
       " 'CDASSIST',\n",
       " 'CDHELP',\n",
       " 'CDSOCIAL',\n",
       " 'CDDISCUS',\n",
       " 'WTCHSALT',\n",
       " 'LONGWTCH',\n",
       " 'DRADVISE',\n",
       " 'ASTHMAGE',\n",
       " 'ASATTACK',\n",
       " 'ASERVIST',\n",
       " 'ASDRVIST',\n",
       " 'ASRCHKUP',\n",
       " 'ASACTLIM',\n",
       " 'ASYMPTOM',\n",
       " 'ASNOSLEP',\n",
       " 'ASTHMED3',\n",
       " 'ASINHALR',\n",
       " 'HAREHAB1',\n",
       " 'STREHAB1',\n",
       " 'CVDASPRN',\n",
       " 'ASPUNSAF',\n",
       " 'RLIVPAIN',\n",
       " 'RDUCHART',\n",
       " 'RDUCSTRK',\n",
       " 'ARTTODAY',\n",
       " 'ARTHWGT',\n",
       " 'ARTHEXER',\n",
       " 'ARTHEDU',\n",
       " 'TETANUS',\n",
       " 'HPVADVC2',\n",
       " 'HPVADSHT',\n",
       " 'SHINGLE2',\n",
       " 'HADMAM',\n",
       " 'HOWLONG',\n",
       " 'HADPAP2',\n",
       " 'LASTPAP2',\n",
       " 'HPVTEST',\n",
       " 'HPLSTTST',\n",
       " 'HADHYST2',\n",
       " 'PROFEXAM',\n",
       " 'LENGEXAM',\n",
       " 'BLDSTOOL',\n",
       " 'LSTBLDS3',\n",
       " 'HADSIGM3',\n",
       " 'HADSGCO1',\n",
       " 'LASTSIG3',\n",
       " 'PCPSAAD2',\n",
       " 'PCPSADI1',\n",
       " 'PCPSARE1',\n",
       " 'PSATEST1',\n",
       " 'PSATIME',\n",
       " 'PCPSARS1',\n",
       " 'PCPSADE1',\n",
       " 'PCDMDECN',\n",
       " 'SCNTMNY1',\n",
       " 'SCNTMEL1',\n",
       " 'SCNTPAID',\n",
       " 'SCNTWRK1',\n",
       " 'SCNTLPAD',\n",
       " 'SCNTLWK1',\n",
       " 'SXORIENT',\n",
       " 'TRNSGNDR',\n",
       " 'RCSGENDR',\n",
       " 'RCSRLTN2',\n",
       " 'CASTHDX2',\n",
       " 'CASTHNO2',\n",
       " 'EMTSUPRT',\n",
       " 'LSATISFY',\n",
       " 'ADPLEASR',\n",
       " 'ADDOWN',\n",
       " 'ADSLEEP',\n",
       " 'ADENERGY',\n",
       " 'ADEAT1',\n",
       " 'ADFAIL',\n",
       " 'ADTHINK',\n",
       " 'ADMOVE',\n",
       " 'MISTMNT',\n",
       " 'ADANXEV',\n",
       " 'QSTVER',\n",
       " 'QSTLANG',\n",
       " 'MSCODE',\n",
       " '_STSTR',\n",
       " '_STRWT',\n",
       " '_RAWRAKE',\n",
       " '_WT2RAKE',\n",
       " '_CHISPNC',\n",
       " '_CRACE1',\n",
       " '_CPRACE',\n",
       " '_CLLCPWT',\n",
       " '_DUALUSE',\n",
       " '_DUALCOR',\n",
       " '_LLCPWT',\n",
       " '_RFHLTH',\n",
       " '_HCVU651',\n",
       " '_RFHYPE5',\n",
       " '_CHOLCHK',\n",
       " '_RFCHOL',\n",
       " '_LTASTH1',\n",
       " '_CASTHM1',\n",
       " '_ASTHMS1',\n",
       " '_DRDXAR1',\n",
       " '_PRACE1',\n",
       " '_MRACE1',\n",
       " '_HISPANC',\n",
       " '_RACE',\n",
       " '_RACEG21',\n",
       " '_RACEGR3',\n",
       " '_RACE_G1',\n",
       " '_AGEG5YR',\n",
       " '_AGE65YR',\n",
       " '_AGE80',\n",
       " '_AGE_G',\n",
       " 'HTIN4',\n",
       " 'HTM4',\n",
       " 'WTKG3',\n",
       " '_BMI5',\n",
       " '_BMI5CAT',\n",
       " '_RFBMI5',\n",
       " '_CHLDCNT',\n",
       " '_EDUCAG',\n",
       " '_INCOMG',\n",
       " '_SMOKER3',\n",
       " '_RFSMOK3',\n",
       " 'DRNKANY5',\n",
       " 'DROCDY3_',\n",
       " '_RFBING5',\n",
       " '_DRNKWEK',\n",
       " '_RFDRHV5',\n",
       " 'FTJUDA1_',\n",
       " 'FRUTDA1_',\n",
       " 'BEANDAY_',\n",
       " 'GRENDAY_',\n",
       " 'ORNGDAY_',\n",
       " 'VEGEDA1_',\n",
       " '_MISFRTN',\n",
       " '_MISVEGN',\n",
       " '_FRTRESP',\n",
       " '_VEGRESP',\n",
       " '_FRUTSUM',\n",
       " '_VEGESUM',\n",
       " '_FRTLT1',\n",
       " '_VEGLT1',\n",
       " '_FRT16',\n",
       " '_VEG23',\n",
       " '_FRUITEX',\n",
       " '_VEGETEX',\n",
       " '_TOTINDA',\n",
       " 'METVL11_',\n",
       " 'METVL21_',\n",
       " 'MAXVO2_',\n",
       " 'FC60_',\n",
       " 'ACTIN11_',\n",
       " 'ACTIN21_',\n",
       " 'PADUR1_',\n",
       " 'PADUR2_',\n",
       " 'PAFREQ1_',\n",
       " 'PAFREQ2_',\n",
       " '_MINAC11',\n",
       " '_MINAC21',\n",
       " 'STRFREQ_',\n",
       " 'PAMISS1_',\n",
       " 'PAMIN11_',\n",
       " 'PAMIN21_',\n",
       " 'PA1MIN_',\n",
       " 'PAVIG11_',\n",
       " 'PAVIG21_',\n",
       " 'PA1VIGM_',\n",
       " '_PACAT1',\n",
       " '_PAINDX1',\n",
       " '_PA150R2',\n",
       " '_PA300R2',\n",
       " '_PA30021',\n",
       " '_PASTRNG',\n",
       " '_PAREC1',\n",
       " '_PASTAE1',\n",
       " '_LMTACT1',\n",
       " '_LMTWRK1',\n",
       " '_LMTSCL1',\n",
       " '_RFSEAT2',\n",
       " '_RFSEAT3',\n",
       " '_FLSHOT6',\n",
       " '_PNEUMO2',\n",
       " '_AIDTST3']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{col:i for i,col in enumerate(list(x_tr.columns)[1:])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intresting_features=[\"GENHLTH\",\"POORHLTH\",\"HLTHPLN1\",\"CHECKUP1\",\"BPMEDS\",\"TOLDHI2\",\"CVDSTRK3\",\"LADULT\",\"PHYSHLTH\",\"MENTHLTH\",\n",
    "                     \"HLTHPLN1\",\"CHECKUP1\",\"TOLDHI2\",\"CVDSTRK3\",\"CHCOCNCR\",\"CHCCOPD1\",\"HAVARTH3\",\"CHCKIDNY\",\"DIABETE3\",\"SEX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_ever_told={1:1,2:0,7:0.5,9:0}\n",
    "dico_transfos={\"GENHLTH\":{7:4,9:4},\"POORHLTH\":{88:0,77:0,99:0},\"HLTHPLN1\":{1:1, 2:0,7:0.5,9:0},\"CHECKUP1\":{1:1,2:2,3:3,4:4,8:15,7:0.5,9:0},\"BPMEDS\":{1:1,2:0,7:0.5,9:0},\"TOLDHI2\":{1:1,2:0,7:0.5,9:0},\n",
    "               \"CVDCRHD4\":{1:1,2:0,7:0.5,9:0},\"LADULT\":{2:0},\"PHYSHLTH\":{88:0,77:np.nan,99:np.nan},\"MENTHLTH\":{88:0,77:0,99:0},\"HLTHPLN1\":dico_ever_told,\" CHECKUP1\":{3:5,4:10,7:2,8:100,9:3},\n",
    "               \"CVDSTRK3\":dico_ever_told,\"TOLDHI2\":dico_ever_told,\"CHCOCNCR\":dico_ever_told,\"CHCCOPD1\":dico_ever_told,\"CVDSTRK3\":dico_ever_told,\"HAVARTH3\":dico_ever_told,\n",
    "               \"CHCKIDNY\":dico_ever_told,\"DIABETE3\":dico_ever_told,\"SEX\":{2:0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr2=x_tr.copy()\n",
    "for col in intresting_features:\n",
    "    x_tr2[col].replace(dico_transfos[col],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_cleaned=x_tr2[intresting_features].copy()\n",
    "fill_mean=False\n",
    "if fill_mean:\n",
    "    for col in intresting_features:\n",
    "        x_tr_cleaned[col]=x_tr_cleaned[col].fillna(x_tr_cleaned[col].mean())\n",
    "else:\n",
    "    x_tr_cleaned=x_tr_cleaned.fillna(0).copy()\n",
    "#x_tr_cleaned=x_tr2.fillna(0).copy()\n",
    "y_tr_cleaned=y_tr.copy()\n",
    "# x_tr_cleaned=x_tr[intresting_features+[\"Id\"]].dropna()\n",
    "# not_na_ids=x_tr_cleaned[\"Id\"].values\n",
    "# y_tr_cleaned=y_tr.loc[y_tr[\"Id\"].isin(not_na_ids)]\n",
    "# x_tr_cleaned=x_tr_cleaned[intresting_features]\n",
    "y_tr_cleaned=y_tr_cleaned[\"_MICHD\"]\n",
    "y_tr_cleaned.replace({-1:0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(x_tr_cleaned.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135, 20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_cleaned.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tra, x_val, y_tra, y_val=split_data(x_tr_cleaned.values,y_tr_cleaned.values.ravel(),ratio=0.75,seed=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(x,w,threshold,apply_sigmoid):\n",
    "    w2=w.ravel()\n",
    "    y_pred=x.dot(w2.T)\n",
    "    if threshold==None:\n",
    "        threshold=0.5\n",
    "    if apply_sigmoid:\n",
    "        y_pred=sigmoid(y_pred)\n",
    "    y_pred=np.array([0 if prediction<threshold else 1 for prediction in y_pred])\n",
    "    return y_pred\n",
    "def compute_scores(x,w,y,threshold=None,apply_sigmoid=False):\n",
    "    y_pred=make_predictions(x,w,threshold,apply_sigmoid) \n",
    "    TP=np.sum(np.logical_and(y_pred==1,y==1))\n",
    "    FP=np.sum(np.logical_and(y_pred==1,y==0))\n",
    "    FN=np.sum(np.logical_and(y_pred==0,y==1))\n",
    "    precision=TP/(TP+FP)\n",
    "    recall=TP/(TP+FN)\n",
    "    f1=2*precision*recall/(precision+recall)\n",
    "    return precision,recall,f1\n",
    "    \n",
    "x_tra_scaled=(x_tra-np.mean(x_tra,axis=0)[None,:])/np.std(x_tra,axis=0)\n",
    "x_val_scaled=(x_val-np.mean(x_tra,axis=0)[None,:])/np.std(x_tra,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08872373537693874"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_tra==1)[0].shape[0]/y_tra.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_ones = int(0.3 * len(y_tra))\n",
    "existing_ones_indices = np.where(y_tra == 1)[0]\n",
    "zeros_indices = np.where(y_tra == 0)[0]\n",
    "ones_indices = np.random.choice(existing_ones_indices, required_ones - len(existing_ones_indices), replace=True)\n",
    "zeros_indices=np.random.choice(zeros_indices,len(y_tra)-required_ones,replace=False)\n",
    "result_indices = np.concatenate([existing_ones_indices, ones_indices,zeros_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tra_scaled1=x_tra_scaled[result_indices].copy()\n",
    "y_tra1=y_tra[result_indices].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NN model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7175\n",
      "learning rate: 0.3\n",
      "Epoch 0, F1 Score: 0.5983\n",
      "Epoch 100, Loss: 0.4978\n",
      "learning rate: 0.23213428124999994\n",
      "Epoch 200, Loss: 0.5192\n",
      "learning rate: 0.1796210817715136\n",
      "Epoch 300, Loss: 0.4593\n",
      "learning rate: 0.11916429553746546\n",
      "Epoch 400, Loss: 0.4839\n",
      "learning rate: 0.11320608076059219\n",
      "Epoch 500, Loss: 0.4601\n",
      "learning rate: 0.08759670730163172\n",
      "Epoch 600, Loss: 0.4836\n",
      "learning rate: 0.07905602833972261\n",
      "Epoch 700, Loss: 0.4578\n",
      "learning rate: 0.06778066229776968\n",
      "Epoch 800, Loss: 0.4652\n",
      "learning rate: 0.04496707621464464\n",
      "Epoch 900, Loss: 0.4537\n",
      "learning rate: 0.03662596462105439\n",
      "Epoch 1000, Loss: 0.4539\n",
      "learning rate: 0.02983207709612768\n",
      "Epoch 1000, F1 Score: 0.6153\n",
      "Epoch 1100, Loss: 0.4665\n",
      "learning rate: 0.028340473241321294\n",
      "Epoch 1200, Loss: 0.4608\n",
      "learning rate: 0.025577277100292468\n",
      "Epoch 1300, Loss: 0.4723\n",
      "learning rate: 0.02308349258301395\n",
      "Epoch 1400, Loss: 0.4512\n",
      "learning rate: 0.017861566531658827\n",
      "Epoch 1500, Loss: 0.4517\n",
      "learning rate: 0.013820939696085584\n",
      "Epoch 1600, Loss: 0.4523\n",
      "learning rate: 0.011849728171931376\n",
      "Epoch 1700, Loss: 0.4574\n",
      "learning rate: 0.010694379675168066\n",
      "Epoch 1800, Loss: 0.4506\n",
      "learning rate: 0.007861351774480864\n",
      "Epoch 1900, Loss: 0.4517\n",
      "learning rate: 0.004471537614742426\n",
      "Epoch 2000, Loss: 0.4512\n",
      "learning rate: 0.003459990567601907\n",
      "Epoch 2000, F1 Score: 0.5920\n",
      "Epoch 2100, Loss: 0.4509\n",
      "learning rate: 0.0021806570128838168\n",
      "Epoch 2200, Loss: 0.4503\n",
      "learning rate: 0.001374357795181855\n",
      "Epoch 2300, Loss: 0.4503\n",
      "learning rate: 0.001\n",
      "Epoch 2400, Loss: 0.4504\n",
      "learning rate: 0.001\n",
      "Epoch 2500, Loss: 0.4517\n",
      "learning rate: 0.001\n",
      "Epoch 2600, Loss: 0.4502\n",
      "learning rate: 0.001\n",
      "Epoch 2700, Loss: 0.4503\n",
      "learning rate: 0.001\n",
      "Epoch 2800, Loss: 0.4507\n",
      "learning rate: 0.001\n",
      "Epoch 2900, Loss: 0.4502\n",
      "learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "input_size = x_tra_scaled.shape[1]\n",
    "hidden_size = 16\n",
    "output_size = 1\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the model\n",
    "model.train(x_tra_scaled1, y_tra1.reshape(-1,1), epochs=3000,batching=True,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6045310690283505\n",
      "0.3970527904954134\n",
      "0.39395580056110646\n"
     ]
    }
   ],
   "source": [
    "print(model.compute_f1_score(x_tra_scaled1,y_tra1.reshape(-1,1)))\n",
    "print(model.compute_f1_score(x_tra_scaled,y_tra.reshape(-1,1)))\n",
    "print(model.compute_f1_score(x_val_scaled,y_val.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8617\n",
      "learning rate: 0.3\n",
      "Epoch 0, F1 Score: 0.5893\n",
      "Epoch 100, Loss: 1.1485\n",
      "learning rate: 0.24435187499999994\n",
      "Epoch 200, Loss: 0.8364\n",
      "learning rate: 0.20950118882812493\n",
      "Epoch 300, Loss: 0.4935\n",
      "learning rate: 0.1389873690479259\n",
      "Epoch 400, Loss: 0.5399\n",
      "learning rate: 0.10754577672256257\n",
      "Epoch 500, Loss: 0.4658\n",
      "learning rate: 0.08321687193655013\n",
      "Epoch 600, Loss: 0.4576\n",
      "learning rate: 0.058113445337550265\n",
      "Epoch 700, Loss: 0.4578\n",
      "learning rate: 0.03855364696953094\n",
      "Epoch 800, Loss: 0.5230\n",
      "learning rate: 0.03140218641697651\n",
      "Epoch 900, Loss: 0.4588\n",
      "learning rate: 0.02308349258301395\n",
      "Epoch 1000, Loss: 0.4698\n",
      "learning rate: 0.021929317953863253\n",
      "Epoch 1000, F1 Score: 0.6492\n",
      "Epoch 1100, Loss: 0.4764\n",
      "learning rate: 0.017861566531658827\n",
      "Epoch 1200, Loss: 0.4566\n",
      "learning rate: 0.014548357574826932\n",
      "Epoch 1300, Loss: 0.4588\n",
      "learning rate: 0.008275107131032489\n",
      "Epoch 1400, Loss: 0.4529\n",
      "learning rate: 0.006403120153763253\n",
      "Epoch 1500, Loss: 0.4526\n",
      "learning rate: 0.0032869910392218115\n",
      "Epoch 1600, Loss: 0.4533\n",
      "learning rate: 0.0019680429541276447\n",
      "Epoch 1700, Loss: 0.4524\n",
      "learning rate: 0.001305639905422762\n",
      "Epoch 1800, Loss: 0.4531\n",
      "learning rate: 0.0010634518632162485\n",
      "Epoch 1900, Loss: 0.4538\n",
      "learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "input_size = x_tra_scaled.shape[1]\n",
    "hidden_size = 16\n",
    "output_size = 1\n",
    "model2 = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the model\n",
    "model2.train(x_tra_scaled1, y_tra1.reshape(-1,1), epochs=2000,batching=True,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6013986013986014\n",
      "0.3947952821963778\n",
      "0.390928405553634\n"
     ]
    }
   ],
   "source": [
    "print(model2.compute_f1_score(x_tra_scaled1,y_tra1.reshape(-1,1)))\n",
    "print(model2.compute_f1_score(x_tra_scaled,y_tra.reshape(-1,1)))\n",
    "print(model2.compute_f1_score(x_val_scaled,y_val.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7281\n",
      "Epoch 0, F1 Score: 0.5904\n",
      "Epoch 100, Loss: 1.0362\n",
      "Epoch 200, Loss: 0.4790\n",
      "Epoch 300, Loss: 0.6093\n",
      "Epoch 400, Loss: 0.4745\n",
      "Epoch 500, Loss: 0.4682\n",
      "Epoch 600, Loss: 0.4897\n",
      "Epoch 700, Loss: 0.4666\n",
      "Epoch 800, Loss: 0.5118\n",
      "Epoch 900, Loss: 0.4573\n",
      "Epoch 1000, Loss: 0.4683\n",
      "Epoch 1000, F1 Score: 0.6438\n",
      "Epoch 1100, Loss: 0.4528\n",
      "Epoch 1200, Loss: 0.4602\n",
      "Epoch 1300, Loss: 0.4564\n",
      "Epoch 1400, Loss: 0.4834\n",
      "Epoch 1500, Loss: 0.4525\n",
      "Epoch 1600, Loss: 0.4559\n",
      "Epoch 1700, Loss: 0.4536\n",
      "Epoch 1800, Loss: 0.4556\n",
      "Epoch 1900, Loss: 0.4530\n",
      "Epoch 2000, Loss: 0.4517\n",
      "Epoch 2000, F1 Score: 0.6031\n",
      "Epoch 2100, Loss: 0.4515\n",
      "Epoch 2200, Loss: 0.4537\n",
      "Epoch 2300, Loss: 0.4519\n",
      "Epoch 2400, Loss: 0.4514\n",
      "Epoch 2500, Loss: 0.4513\n",
      "Epoch 2600, Loss: 0.4514\n",
      "Epoch 2700, Loss: 0.4513\n",
      "Epoch 2800, Loss: 0.4512\n",
      "Epoch 2900, Loss: 0.4524\n"
     ]
    }
   ],
   "source": [
    "input_size = x_tra_scaled.shape[1]\n",
    "hidden_size = 16\n",
    "output_size = 1\n",
    "model3 = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the model\n",
    "model3.train(x_tra_scaled1, y_tra1.reshape(-1,1), epochs=3000,batching=True,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.603412727151799\n",
      "0.39604350826459855\n",
      "0.39403515702152875\n"
     ]
    }
   ],
   "source": [
    "print(model3.compute_f1_score(x_tra_scaled1,y_tra1.reshape(-1,1)))\n",
    "print(model3.compute_f1_score(x_tra_scaled,y_tra.reshape(-1,1)))\n",
    "print(model3.compute_f1_score(x_val_scaled,y_val.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7273\n",
      "learning rate: 0.3\n",
      "Epoch 0, F1 Score: 0.0000\n",
      "Epoch 100, Loss: 0.5250\n",
      "learning rate: 0.27075\n",
      "Epoch 200, Loss: 0.5520\n",
      "learning rate: 0.18907482291738273\n",
      "Epoch 300, Loss: 0.7252\n",
      "learning rate: 0.1389873690479259\n",
      "Epoch 400, Loss: 0.5555\n",
      "learning rate: 0.11916429553746546\n",
      "Epoch 500, Loss: 0.4840\n",
      "learning rate: 0.08759670730163172\n",
      "Epoch 600, Loss: 0.4569\n",
      "learning rate: 0.07905602833972261\n",
      "Epoch 700, Loss: 0.5831\n",
      "learning rate: 0.058113445337550265\n",
      "Epoch 800, Loss: 0.4821\n",
      "learning rate: 0.05244738441713911\n",
      "Epoch 900, Loss: 0.4817\n",
      "learning rate: 0.04496707621464464\n"
     ]
    }
   ],
   "source": [
    "input_size = x_tra_scaled.shape[1]\n",
    "hidden_size = 16\n",
    "output_size = 1\n",
    "model4 = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the model\n",
    "model4.train(x_tra_scaled1, y_tra1.reshape(-1,1), epochs=1000,batching=True,batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5908772464209565\n",
      "0.3977957995425244\n",
      "0.39393781573876363\n"
     ]
    }
   ],
   "source": [
    "the_model=model4\n",
    "print(the_model.compute_f1_score(x_tra_scaled1,y_tra1.reshape(-1,1)))\n",
    "print(the_model.compute_f1_score(x_tra_scaled,y_tra.reshape(-1,1)))\n",
    "print(the_model.compute_f1_score(x_val_scaled,y_val.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids=load_csv_data(\"data\")\n",
    "x_test=pd.read_csv(\"data/x_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test2=x_test.copy()\n",
    "fill_mean=False\n",
    "for col in intresting_features:\n",
    "    x_test2[col].replace(dico_transfos[col],inplace=True)\n",
    "\n",
    "        \n",
    "    \n",
    "if fill_mean:\n",
    "    x_test_cleaned=x_test2[intresting_features].copy()\n",
    "    for col in intresting_features:\n",
    "        x_test_cleaned[col]=x_test_cleaned[col].fillna(x_tr_cleaned[col].mean())\n",
    "else:\n",
    "    x_test_cleaned=x_test2[intresting_features].fillna(0).copy()\n",
    "x_test_scaled=(x_test_cleaned-np.mean(x_tra,axis=0)[None,:])/np.std(x_tra,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ..., -1., -1.,  1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=the_model.predict(x_test_scaled)\n",
    "predictions=predictions.reshape(x_test_scaled.shape[0])\n",
    "predictions[np.where(predictions==0)[0]]=-1\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17333,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(predictions==1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids,predictions,name=\"predictions_nn_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
