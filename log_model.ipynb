{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "import os\n",
    "import csv\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(DATA_FOLDER + 'x_train.pickle', 'rb') as f:\n",
    "        x_train = pickle.load(f)\n",
    "    with open(DATA_FOLDER + 'x_test.pickle', 'rb') as f:\n",
    "        x_test = pickle.load(f)\n",
    "    with open(DATA_FOLDER + 'y_train.pickle', 'rb') as f:\n",
    "        y_train = pickle.load(f)\n",
    "    with open(DATA_FOLDER + 'test_ids.pickle', 'rb') as f:\n",
    "        test_ids = pickle.load(f)\n",
    "    with open(DATA_FOLDER + 'names_map.pickle', 'rb') as f:\n",
    "        names_map = pickle.load(f)\n",
    "except:\n",
    "    x_train, x_test, y_train, train_ids, test_ids = load_csv_data(DATA_FOLDER, sub_sample=False)\n",
    "\n",
    "    names = np.genfromtxt(DATA_FOLDER + 'x_train.csv', delimiter=\",\", dtype=str, max_rows=1)\n",
    "    names = np.delete(names, 0)\n",
    "    names_map = {}\n",
    "    for i in range(len(names)):\n",
    "        names_map[names[i]] = i\n",
    "\n",
    "    with open(DATA_FOLDER + 'x_train.pickle', 'wb') as f:\n",
    "        pickle.dump(x_train, f)\n",
    "\n",
    "    with open(DATA_FOLDER + 'x_test.pickle', 'wb') as f:\n",
    "        pickle.dump(x_test, f)\n",
    "\n",
    "    with open(DATA_FOLDER + 'y_train.pickle', 'wb') as f:\n",
    "        pickle.dump(y_train, f)\n",
    "\n",
    "    with open(DATA_FOLDER + 'test_ids.pickle', 'wb') as f:\n",
    "        pickle.dump(test_ids, f)\n",
    "    \n",
    "    with open(DATA_FOLDER + 'names_map.pickle', 'wb') as f:\n",
    "        pickle.dump(names_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train == -1, 0, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = x_train.copy()\n",
    "y_tr = y_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = preprocessing.clean_data(x_tr, y_tr, names_map=names_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([501., 510., 507., ..., 502., 510., 508.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr[:, names_map[\"HEIGHT3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([224.54, 250.4 , 241.78, ..., 228.08, 250.4 , 244.32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[:, names_map[\"HEIGHT3\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM : 501 = 5ft 1in = 154.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 110.,  200., 9999., ...,  320.,  250.,  150.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr[:, names_map[\"WEIGHT2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  49.89512,   90.7184 , 9999.     , ...,  145.14944,  113.398  ,\n",
       "         68.0388 ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[:, names_map[\"WEIGHT2\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEIGHT OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the weights to kg, and asigning lacking answers to NaN\n",
    "array = x_tr[:, names_map[\"WEIGHT2\"]]\n",
    "pounds_indices  = np.where(((array >= 50)*(array <= 999))==True)\n",
    "kg_indices      = np.where(((array >= 9000)*(array <= 9998))==True)\n",
    "none_indices    = np.where(((array == 7777) + (array == 9998))==True)\n",
    "\n",
    "x_tr[:, names_map[\"WEIGHT2\"]][pounds_indices] = 0.453592 * x_tr[:, names_map[\"WEIGHT2\"]][pounds_indices]\n",
    "x_tr[:, names_map[\"WEIGHT2\"]][kg_indices] = x_tr[:, names_map[\"WEIGHT2\"]][kg_indices]%9000\n",
    "x_tr[:, names_map[\"WEIGHT2\"]][none_indices] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the height to meters, and asigning lacking answers to NaN\n",
    "array = x_tr[:, names_map[\"HEIGHT3\"]]\n",
    "imperial_indices    = np.where(((array >= 200)*(array <= 711))==True)\n",
    "cm_indices          = np.where(((array >= 9000)*(array <= 9998))==True)\n",
    "none_indices        = np.where((array == 9998)==True)\n",
    "\n",
    "x_tr[:, names_map[\"HEIGHT3\"]][imperial_indices] = 0.3048 * x_tr[:, names_map[\"HEIGHT3\"]][imperial_indices]//100 + 0.0254 * x_tr[:, names_map[\"HEIGHT3\"]][imperial_indices]%100\n",
    "x_tr[:, names_map[\"HEIGHT3\"]][cm_indices] = x_tr[:, names_map[\"HEIGHT3\"]][cm_indices]%9000\n",
    "x_tr[:, names_map[\"HEIGHT3\"]][none_indices] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting FLSHTMY2 to months, and asigning lacking answers to NaN\n",
    "array = x_tr[:, names_map[\"FLSHTMY2\"]]\n",
    "none_indices = np.where(((array == 777777) + (array == 999999))==True)\n",
    "days_indices = np.where(((array >= 12014)*(array <= 122015))==True)\n",
    "\n",
    "x_tr[:, names_map[\"FLSHTMY2\"]][none_indices] = np.nan\n",
    "x_tr[:, names_map[\"FLSHTMY2\"]][days_indices] = x_tr[:, names_map[\"FLSHTMY2\"]][days_indices]//10000 + 12*x_tr[:, names_map[\"FLSHTMY2\"]][days_indices]%10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a few functions used to clean and scale the data properly\n",
    "def frequency_scaler(df, col):\n",
    "    \n",
    "    times_per_day   = np.where(((df[:, names_map[col]]>=101)*(df[:, names_map[col]]<=199))==True)\n",
    "    times_per_week  = np.where(((df[:, names_map[col]]>=201)*(df[:, names_map[col]]<=299))==True)\n",
    "    times_per_month = np.where(((df[:, names_map[col]]>=301)*(df[:, names_map[col]]<=399))==True)\n",
    "    none_indices    = np.where(((df[:, names_map[col]] == 777) + (df[:, names_map[col]] == 999))==True)\n",
    "\n",
    "    df[:, names_map[col]][times_per_day]                = df[:, names_map[col]][times_per_day]%100\n",
    "    df[:, names_map[col]][times_per_week]               = (df[:, names_map[col]][times_per_week]%200)/7\n",
    "    df[:, names_map[col]][times_per_month]              = (df[:, names_map[col]][times_per_month]%300)/30\n",
    "    df[:, names_map[col]][df[:, names_map[col]]==300]   = 1/30\n",
    "    df[:, names_map[col]][df[:, names_map[col]]==555]   = 0\n",
    "    df[:, names_map[col]][none_indices]                 = np.nan\n",
    "\n",
    "def weekly_frequency_scaler(df, col):\n",
    "    times_per_week  = np.where(((df[:, names_map[col]]>=101)*(df[:, names_map[col]]<=199))==True)\n",
    "    times_per_month = np.where(((df[:, names_map[col]]>=201)*(df[:, names_map[col]]<=299))==True)\n",
    "    none_indices    = np.where(((df[:, names_map[col]] == 777) + (df[:, names_map[col]] == 999))==True)\n",
    "\n",
    "    df[:, names_map[col]][times_per_week]                   = (df[:, names_map[col]][times_per_week]%100)/7\n",
    "    df[:, names_map[col]][times_per_month]                  = (df[:, names_map[col]][times_per_month]%200)/30\n",
    "    df[:, names_map[col]][df[:, names_map[col]]==888]       = 0\n",
    "    df[:, names_map[col]][none_indices]                     = np.nan\n",
    "\n",
    "def hours_to_minutes(df, col):\n",
    "    hour_indices    = np.where(( (df[:, names_map[col]]>=1)*(df[:, names_map[col]]<=759) + (df[:, names_map[col]]>=800)*(df[:, names_map[col]]<=959))==True)\n",
    "    none_indices    = np.where(((df[:, names_map[col]]==777) + (df[:, names_map[col]]==999))==True)\n",
    "    \n",
    "    df[:, names_map[col]][hour_indices] = 60*df[:, names_map[col]][hour_indices]//100 + df[:, names_map[col]][hour_indices]%100                                                 \n",
    "    df[:, names_map[col]][none_indices] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix the frequency scales of the following columns\n",
    "frequency_scaler(x_tr, \"FRUITJU1\")\n",
    "frequency_scaler(x_tr, \"FRUIT1\")\n",
    "frequency_scaler(x_tr, \"FVBEANS\")\n",
    "frequency_scaler(x_tr, \"FVGREEN\")\n",
    "frequency_scaler(x_tr, \"FVORANG\")\n",
    "frequency_scaler(x_tr, \"VEGETAB1\")\n",
    "\n",
    "hours_to_minutes(x_tr, \"EXERHMM1\")\n",
    "hours_to_minutes(x_tr, \"EXERHMM2\")\n",
    "\n",
    "weekly_frequency_scaler(x_tr, \"ALCDAY5\")\n",
    "weekly_frequency_scaler(x_tr, \"EXEROFT1\")\n",
    "weekly_frequency_scaler(x_tr, \"EXEROFT2\")\n",
    "weekly_frequency_scaler(x_tr, \"STRENGTH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation hashmap\n",
    "dico_transfos={\"GENHLTH\":{7:np.nan,8:np.nan,9:np.nan},\"POORHLTH\":{88:0,77:np.nan,99:np.nan},\"HLTHPLN1\":{7:np.nan,9:np.nan},\"CHECKUP1\":{8:15,7:np.nan,9:np.nan},\n",
    "               \"BPMEDS\":{7:np.nan,9:np.nan}, \"TOLDHI2\":{7:np.nan,9:np.nan}, \"PHYSHLTH\":{88:0,77:np.nan,99:np.nan},\n",
    "               \"MENTHLTH\":{88:0,77:np.nan,99:np.nan}, \"CVDSTRK3\":{7:np.nan, 9:np.nan}, \"HLTHPLN1\":{9:np.nan}, \"CHCOCNCR\":{7:np.nan, 9:np.nan},\n",
    "               \"HAVARTH3\":{7:np.nan, 9:np.nan}, \"CHCKIDNY\":{7:np.nan, 9:np.nan}, \"DIABETE3\":{7:np.nan, 9:np.nan}, \"CHCCOPD1\":{7:np.nan, 9:np.nan},\n",
    "               \"ASTHMA3\":{7:np.nan, 9:np.nan}, \"ASTHNOW\":{7:np.nan, 9:np.nan}, \"CHCSCNCR\":{7:np.nan, 9:np.nan}, \"ADDEPEV2\":{7:np.nan, 9:np.nan},\n",
    "               \"DIABAGE2\":{98:np.nan, 99:np.nan}, \"EDUCA\":{9:np.nan}, \"INCOME2\":{77:np.nan, 99:np.nan}, \"QLACTLM2\":{7:np.nan, 9:np.nan},\n",
    "               \"USEEQUIP\":{7:np.nan, 9:np.nan}, \"BLIND\":{7:np.nan, 9:np.nan}, \"DECIDE\":{7:np.nan, 9:np.nan}, \"DIFFWALK\":{7:np.nan, 9:np.nan},\n",
    "               \"DIFFDRES\":{7:np.nan, 9:np.nan}, \"DIFFALON\":{7:np.nan, 9:np.nan}, \"SMOKE100\":{7:np.nan, 9:np.nan}, \"SMOKDAY2\":{7:np.nan, 9:np.nan},\n",
    "               \"USENOW3\":{7:np.nan, 9:np.nan}, \"AVEDRNK2\":{77:np.nan, 99:np.nan}, \"DRNK3GE5\":{77:np.nan, 88:np.nan, 99:np.nan}, \"MAXDRNKS\":{77:np.nan, 99:np.nan},\n",
    "               \"EXERANY2\":{7:np.nan, 9:np.nan}, \"EXERHMM1\":{777:np.nan, 999:np.nan}, \"SEATBELT\":{7:np.nan, 8:np.nan, 9:np.nan}, \"PNEUVAC3\":{7:np.nan, 9:np.nan},\n",
    "               \"ARTHDIS2\":{7:np.nan, 9:np.nan}, \"ARTHSOCL\":{7:np.nan, 9:np.nan}, \"JOINPAIN\":{77:np.nan, 99:np.nan}, \"ARTHEDU\":{7:np.nan, 9:np.nan}, \"FLUSHOT6\":{7:np.nan, 9:np.nan},\n",
    "               \"DOCTDIAB\":{88:0, 77:np.nan, 99:np.nan}, \"DIABEYE\":{7:np.nan, 9:np.nan}, \"CRGVMST2\":{7:np.nan, 9:np.nan}, \"VIDFCLT2\":{7:np.nan}, \"VIREDIF3\":{7:np.nan},\n",
    "               \"VICTRCT4\":{7:np.nan}, \"VIGLUMA2\":{7:np.nan}, \"VIMACDG2\":{7:np.nan}, \"CIMEMLOS\":{7:np.nan, 9:np.nan}, \"CDSOCIAL\":{7:np.nan, 9:np.nan}, \"DRADVISE\":{7:np.nan, 9:np.nan},\n",
    "               \"ASTHMAGE\":{97:6, 98:np.nan, 99:np.nan}, \"ASERVIST\":{88:0}, \"CVDASPRN\":{7:np.nan, 9:np.nan}, \"RDUCHART\":{7:np.nan, 9:np.nan}, \"ARTHEXER\":{7:2, 9:2},\n",
    "               \"HPVADVC2\":{7:np.nan, 9:np.nan}, \"HPVADSHT\":{77:np.nan, 99:np.nan}, \"PCPSARE1\":{7:np.nan, 9:np.nan}, \"MISTMNT\":{7:np.nan, 9:np.nan},\n",
    "               \"_CHISPNC\":{9:np.nan}, \"_RFCHOL\":{9:np.nan}, \"_LTASTH1\":{9:1}, \"_CASTHM1\":{9:0}, \"_ASTHMS1\":{9:3}, \"_HISPANC\":{9:2}, \"_AGEG5YR\":{14:np.nan}, \"_CHLDCNT\":{9:np.nan},\n",
    "               \"_EDUCAG\":{9:np.nan}, \"_INCOMG\":{9:np.nan}, \"_SMOKER3\":{9:4}, \"_RFSMOK3\":{9:1}, \"DRNKANY5\":{9:1, 7:1}, \"DROCDY3_\":{900:np.nan},\n",
    "               \"_TOTINDA\":{9:np.nan}, \"_LMTSCL1\":{9:np.nan}, \"_RFSEAT2\":{9:0}, \"_PASTRNG\":{9:2}, \"_PACAT1\":{9:2}, \"STRFREQ_\":{99000:np.nan}\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_temp=x_tr.copy()\n",
    "for col in dico_transfos:\n",
    "    for key in dico_transfos[col].keys():\n",
    "        key_indices = np.where(x_tr_temp[:, names_map[col]]==key)\n",
    "        x_tr_temp[:, names_map[col]][key_indices]=dico_transfos[col][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to NaN in a column of a pandas dataframe\n",
    "def replace_nan(dataframe, column, value):\n",
    "    nan_indices = np.where(np.isnan(dataframe[:, names_map[column]]))\n",
    "    dataframe[:, names_map[column]][nan_indices] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRGVMST2_value    = 6\n",
    "VICTRCT4_value    = 3\n",
    "ARTHEXER_value    = 2\n",
    "HPVADSHT_value    = 0\n",
    "PCPSARE1_value    = 2\n",
    "\n",
    "\n",
    "#list of features to replace NaN with the mean\n",
    "mean_features   = [\"POORHLTH\", \"PHYSHLTH\", \"MENTHLTH\", \"WEIGHT2\", \"DIABAGE2\", \"WEIGHT2\", \"HEIGHT3\", \"FRUITJU1\", \"FRUIT1\", \"FVBEANS\", \"FVGREEN\", \"FVORANG\", \"VEGETAB1\",\n",
    "                   \"EXERHMM1\", \"FLSHTMY2\", \"FTJUDA1_\", \"FRUTDA1_\", \"BEANDAY_\", \"GRENDAY_\", \"ORNGDAY_\", \"VEGEDA1_\", \"STRFREQ_\"]\n",
    "\n",
    "#list of features to replace NaN with the median\n",
    "median_features = [\"GENHLTH\", \"HLTHPLN1\", \"CHECKUP1\", \"BPMEDS\", \"TOLDHI2\", \"CVDSTRK3\", \"CHCKIDNY\", \"CHCOCNCR\", \"HAVARTH3\", \"DIABETE3\", \"CHCCOPD1\", \"ASTHMA3\", \"ASTHNOW\", \"CHCSCNCR\",\n",
    "                   \"ADDEPEV2\", \"EDUCA\", \"INCOME2\", \"QLACTLM2\", \"USEEQUIP\", \"BLIND\", \"DECIDE\", \"DIFFWALK\", \"DIFFDRES\", \"DIFFALON\", \"SMOKE100\", \"SMOKDAY2\", \"USENOW3\", \"ALCDAY5\", \"AVEDRNK2\",\n",
    "                   \"DRNK3GE5\", \"MAXDRNKS\", \"EXERANY2\", \"SEATBELT\", \"PNEUVAC3\", \"ARTHDIS2\", \"ARTHSOCL\", \"JOINPAIN\", \"FLUSHOT6\", \"DOCTDIAB\", \"VIREDIF3\", \"VIGLUMA2\", \"VIMACDG2\", \"CIMEMLOS\",\n",
    "                   \"CDSOCIAL\", \"DRADVISE\", \"HPVADVC2\", \"_CHISPNC\", \"_DRDXAR1\", \"_AGEG5YR\", \"DROCDY3_\", \"_CHLDCNT\", \"_TOTINDA\", \"_LMTSCL1\", \"ARTHEDU\", \"_INCOMG\", \"_EDUCAG\", \"_RFCHOL\", \n",
    "                   \"MISTMNT\", \"DIABEYE\", \"_BMI5\"]\n",
    "\n",
    "#list of specific features\n",
    "value_features  = [\"CRGVMST2\", \"VICTRCT4\", \"ARTHEXER\", \"HPVADSHT\", \"PCPSARE1\"]\n",
    "\n",
    "def replace_nan_by_mean(dataframe, col):\n",
    "    replace_nan(dataframe, feature, dataframe[:, names_map[feature]].mean())\n",
    "\n",
    "def replace_nan_by_median(dataframe, col):\n",
    "    replace_nan(dataframe, feature, np.median(dataframe[:, names_map[feature]]))\n",
    "\n",
    "\n",
    "#replace the NaN with the mean\n",
    "for feature in mean_features:\n",
    "    replace_nan_by_mean(x_tr_temp, feature)\n",
    "\n",
    "#replace the NaN with the median\n",
    "for feature in median_features:\n",
    "    replace_nan_by_median(x_tr_temp, feature)\n",
    "\n",
    "#replace the NaN with a specific value\n",
    "for feature in value_features:\n",
    "    replace_nan(x_tr_temp, feature, eval(feature + \"_value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_cleaned=x_tr[intresting_features].copy()\n",
    "fill_mean=False\n",
    "if fill_mean:\n",
    "    for col in intresting_features:\n",
    "        x_tr_cleaned[col]=x_tr_cleaned[col].fillna(x_tr_cleaned[col].mean())\n",
    "else:\n",
    "    x_tr_cleaned=x_tr.fillna(0).copy()\n",
    "#x_tr_cleaned=x_tr2.fillna(0).copy()\n",
    "y_tr_cleaned=y_tr.copy()\n",
    "# x_tr_cleaned=x_tr[intresting_features+[\"Id\"]].dropna()\n",
    "# not_na_ids=x_tr_cleaned[\"Id\"].values\n",
    "# y_tr_cleaned=y_tr.loc[y_tr[\"Id\"].isin(not_na_ids)]\n",
    "# x_tr_cleaned=x_tr_cleaned[intresting_features]\n",
    "y_tr_cleaned=y_tr_cleaned[\"_MICHD\"]\n",
    "y_tr_cleaned.replace({-1:0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.isnan(x_tr_cleaned.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tra, x_val, y_tra, y_val=split_data(x_tr_cleaned.values,y_tr_cleaned.values.ravel(),ratio=0.75,seed=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(x,w,threshold,apply_sigmoid):\n",
    "    w2=w.ravel()\n",
    "    y_pred=x.dot(w2.T)\n",
    "    if threshold==None:\n",
    "        threshold=0.5\n",
    "    if apply_sigmoid:\n",
    "        y_pred=sigmoid(y_pred)\n",
    "    y_pred=np.array([0 if prediction<threshold else 1 for prediction in y_pred])\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(x,w,y,threshold=None,apply_sigmoid=False):\n",
    "    y_pred=make_predictions(x,w,threshold,apply_sigmoid) \n",
    "    TP=np.sum(np.logical_and(y_pred==1,y==1))\n",
    "    FP=np.sum(np.logical_and(y_pred==1,y==0))\n",
    "    FN=np.sum(np.logical_and(y_pred==0,y==1))\n",
    "    precision=TP/(TP+FP)\n",
    "    recall=TP/(TP+FN)\n",
    "    f1=2*precision*recall/(precision+recall)\n",
    "    return precision,recall,f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tra_scaled=(x_tra-np.mean(x_tra,axis=0)[None,:])/np.std(x_tra,axis=0)\n",
    "x_val_scaled=(x_val-np.mean(x_tra,axis=0)[None,:])/np.std(x_tra,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tra_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opt,loss=logistic_regression(y_tra,x_tra_scaled,initial_w=np.zeros((x_tra.shape[1],1)),max_iters=15,gamma=0.2,gd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{intresting_features[k]:w_opt[k][0] for k in range(len(intresting_features))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_scores(x_val_scaled,w_opt,y_val,threshold=0.58,apply_sigmoid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_l=np.arange(0.3,0.7,0.01)\n",
    "f1_scores=[compute_scores(x_val_scaled,w_opt,y_val,threshold=t,apply_sigmoid=True)[2] for t in thr_l]\n",
    "plt.plot(thr_l,f1_scores,marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"w_log_reg_20f_fillnamean.npy\",w_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids=load_csv_data(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=pd.read_csv(\"data/x_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test2=x_test.copy()\n",
    "fill_mean=True\n",
    "for col in intresting_features:\n",
    "    x_test2[col].replace(dico_transfos[col],inplace=True)\n",
    "\n",
    "        \n",
    "    \n",
    "if fill_mean:\n",
    "    x_test_cleaned=x_test2[intresting_features].copy()\n",
    "    for col in intresting_features:\n",
    "        x_test_cleaned[col]=x_test_cleaned[col].fillna(x_tr_cleaned[col].mean())\n",
    "else:\n",
    "    x_test_cleaned=x_test2[intresting_features].fillna(0).copy()\n",
    "x_test_scaled=(x_test_cleaned-np.mean(x_tra,axis=0)[None,:])/np.std(x_tra,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=make_predictions(x_test_scaled,w_opt,threshold=0.58,apply_sigmoid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(predictions==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[np.where(predictions==0)[0]]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids,predictions,name=\"predictionslog20f_fillnamean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
