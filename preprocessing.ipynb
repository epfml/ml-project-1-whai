{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "import os\n",
    "import csv\n",
    "from helpers import *\n",
    "from neural_network_model import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = pd.read_csv('data/x_train.csv')\n",
    "y0 = pd.read_csv('data/y_train.csv')\n",
    "x_tr = x0.copy(deep=True)\n",
    "y_tr = y0.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Converting the weights to kg, and asigning lacking answers to -1\n",
    "array = x_tr[\"WEIGHT2\"].values\n",
    "pounds_indices  = ((array >= 50)*(array <= 999))\n",
    "kg_indices      = ((array >= 9000)*(array <= 9998))\n",
    "none_indices    = ((array == 7777) + (array == 9998))\n",
    "\n",
    "x_tr[\"WEIGHT2\"][pounds_indices] = 0.453592 * x_tr[\"WEIGHT2\"][pounds_indices]\n",
    "x_tr[\"WEIGHT2\"][kg_indices] = x_tr[\"WEIGHT2\"][kg_indices]%9000\n",
    "x_tr[\"WEIGHT2\"][none_indices] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#converting the height to meters, and asigning lacking answers to -1\n",
    "array = x_tr[\"HEIGHT3\"].values\n",
    "imperial_indices    = ((array >= 200)*(array <= 711))\n",
    "cm_indices          = ((array >= 9000)*(array <= 9998))\n",
    "none_indices        = (array == 9998)\n",
    "\n",
    "x_tr[\"HEIGHT3\"][imperial_indices] = 0.3048 * x_tr[\"HEIGHT3\"][imperial_indices]//100 + 0.0254 * x_tr[\"HEIGHT3\"][imperial_indices]%100\n",
    "x_tr[\"HEIGHT3\"][cm_indices] = x_tr[\"HEIGHT3\"][cm_indices]%9000\n",
    "x_tr[\"HEIGHT3\"][none_indices] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a few functions used to clean and scale the data properly\n",
    "def frequency_scaler(df, col):\n",
    "    times_per_day   = (df[col]>=101)*(df[col]<=199)\n",
    "    times_per_week  = (df[col]>=201)*(df[col]<=299)\n",
    "    times_per_month = (df[col]>=301)*(df[col]<=399)\n",
    "    none_indices    = (df[col] == 777) + (df[col] == 999)\n",
    "\n",
    "    df[col][times_per_day]      = df[col][times_per_day]%100\n",
    "    df[col][times_per_week]     = (df[col][times_per_week]%200)/7\n",
    "    df[col][times_per_month]    = (df[col][times_per_month]%300)/30\n",
    "    df[col][df[col]==300]       = 1/30\n",
    "    df[col][df[col]==555]       = 0\n",
    "    x_tr[col][none_indices] = np.nan\n",
    "\n",
    "def weekly_frequency_scaler(df, col):\n",
    "    times_per_week  = (df[col]>=101)*(df[col]<=199)\n",
    "    times_per_month = (df[col]>=201)*(df[col]<=299)\n",
    "    none_indices    = (df[col] == 777) + (df[col] == 999)\n",
    "\n",
    "    df[col][times_per_week]     = (df[col][times_per_week]%100)/7\n",
    "    df[col][times_per_month]    = (df[col][times_per_month]%200)/30\n",
    "    df[col][df[col]==888]       = 0\n",
    "    df[col][none_indices]       = np.nan\n",
    "\n",
    "def hours_to_minutes(df, col):\n",
    "    df[col][df[col].between(1, 759, \"both\")*df[col].between(800, 959, \"both\")] = 60*df[col][df[col].between(1, 759, \"both\")*df[col].between(800, 959, \"both\")]//100 + df[col][df[col].between(1, 759, \"both\")*df[col].between(800, 959, \"both\")]%100\n",
    "    df[col][(df[col]==777) + (df[col]==999)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#fix the frequency scales of the following columns\n",
    "frequency_scaler(x_tr, \"FRUITJU1\")\n",
    "frequency_scaler(x_tr, \"FRUIT1\")\n",
    "frequency_scaler(x_tr, \"FVBEANS\")\n",
    "frequency_scaler(x_tr, \"FVGREEN\")\n",
    "frequency_scaler(x_tr, \"FVORANG\")\n",
    "frequency_scaler(x_tr, \"VEGETAB1\")\n",
    "\n",
    "hours_to_minutes(x_tr, \"EXERHMM1\")\n",
    "hours_to_minutes(x_tr, \"EXERHMM2\")\n",
    "\n",
    "weekly_frequency_scaler(x_tr, \"ALCDAY5\")\n",
    "weekly_frequency_scaler(x_tr, \"EXEROFT1\")\n",
    "weekly_frequency_scaler(x_tr, \"EXEROFT2\")\n",
    "weekly_frequency_scaler(x_tr, \"STRENGTH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few interesting features\n",
    "intresting_features=[\"GENHLTH\",\"POORHLTH\",\"HLTHPLN1\",\"CHECKUP1\",\"BPMEDS\",\"TOLDHI2\",\"CVDSTRK3\",\"PHYSHLTH\",\"MENTHLTH\",\n",
    "                     \"HLTHPLN1\",\"CVDSTRK3\",\"CHCOCNCR\",\"CHCCOPD1\",\"HAVARTH3\",\"CHCKIDNY\",\"DIABETE3\", \"ASTHMA3\", \"ASTHNOW\",\n",
    "                     \"CHCSCNCR\", \"ADDEPEV2\", \"DIABAGE2\", \"EDUCA\", \"INCOME2\", \"QLACTLM2\", \"USEEQUIP\", \"BLIND\", \"DECIDE\",\n",
    "                     \"DIFFWALK\", \"DIFFDRES\", \"DIFFALON\", \"SMOKE100\"]\n",
    "dico_transfos={\"GENHLTH\":{7:np.nan,8:np.nan,9:np.nan},\"POORHLTH\":{88:0,77:np.nan,99:np.nan},\"HLTHPLN1\":{7:np.nan,9:np.nan},\"CHECKUP1\":{8:15,7:np.nan,9:np.nan},\n",
    "               \"BPMEDS\":{7:np.nan,9:np.nan}, \"TOLDHI2\":{7:np.nan,9:np.nan}, \"PHYSHLTH\":{88:0,77:np.nan,99:np.nan},\n",
    "               \"MENTHLTH\":{88:0,77:np.nan,99:np.nan}, \"CVDSTRK3\":{7:np.nan, 9:np.nan}, \"HLTHPLN1\":{9:np.nan}, \"CHCOCNCR\":{7:np.nan, 9:np.nan},\n",
    "               \"HAVARTH3\":{7:np.nan, 9:np.nan}, \"CHCKIDNY\":{7:np.nan, 9:np.nan}, \"DIABETE3\":{7:np.nan, 9:np.nan}, \"CHCCOPD1\":{7:np.nan, 9:np.nan},\n",
    "               \"ASTHMA3\":{7:np.nan, 9:np.nan}, \"ASTHNOW\":{7:np.nan, 9:np.nan}, \"CHCSCNCR\":{7:np.nan, 9:np.nan}, \"ADDEPEV2\":{7:np.nan, 9:np.nan},\n",
    "               \"DIABAGE2\":{98:np.nan, 99:np.nan}, \"EDUCA\":{9:np.nan}, \"INCOME2\":{77:np.nan, 99:np.nan}, \"QLACTLM2\":{7:np.nan, 9:np.nan},\n",
    "               \"USEEQUIP\":{7:np.nan, 9:np.nan}, \"BLIND\":{7:np.nan, 9:np.nan}, \"DECIDE\":{7:np.nan, 9:np.nan}, \"DIFFWALK\":{7:np.nan, 9:np.nan},\n",
    "               \"DIFFDRES\":{7:np.nan, 9:np.nan}, \"DIFFALON\":{7:np.nan, 9:np.nan}, \"SMOKE100\":{7:np.nan, 9:np.nan}, \"SMOKDAY2\":{7:np.nan, 9:np.nan},\n",
    "               \"USENOW3\":{7:np.nan, 9:np.nan}, \"AVEDRNK2\":{77:np.nan, 99:np.nan}, \"DRNK3GE5\":{77:np.nan, 88:np.nan, 99:np.nan}, \"MAXDRNKS\":{77:np.nan, 99:np.nan},\n",
    "               \"EXERANY2\":{7:np.nan, 9:np.nan}, \"EXERHMM1\":{777:np.nan, 999:np.nan}, \"SEATBELT\":{7:np.nan, 8:np.nan, 9:np.nan}, \"PNEUVAC3\":{7:np.nan, 9:np.nan}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_temp=x_tr.copy()\n",
    "for col in dico_transfos:\n",
    "    x_tr_temp[col].replace(dico_transfos[col],inplace=True)\n",
    "#function to NaN in a column of a pandas dataframe\n",
    "def replace_nan(dataframe, column, value):\n",
    "    dataframe[column].fillna(value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the means we will use to replace the NaN\n",
    "POORHLTH_mean   = x_tr_temp[\"POORHLTH\"][x_tr_temp[\"POORHLTH\"].isin(np.arange(0, 31))].mean()\n",
    "PHYSHLTH_mean   = x_tr_temp[\"PHYSHLTH\"][x_tr_temp[\"PHYSHLTH\"].isin(np.arange(0, 31))].mean()\n",
    "MENTHLTH_mean   = x_tr_temp[\"MENTHLTH\"][x_tr_temp[\"MENTHLTH\"].isin(np.arange(0, 31))].mean()\n",
    "WEIGHT2_mean    = x_tr_temp[\"WEIGHT2\"].mean()\n",
    "DIABAGE2_mean   = x_tr_temp[\"DIABAGE2\"].mean()\n",
    "WEIGHT2_mean    = x_tr_temp[\"WEIGHT2\"].mean()\n",
    "HEIGHT3_mean    = x_tr_temp[\"HEIGHT3\"].mean()\n",
    "FRUITJU1_mean   = x_tr_temp[\"FRUITJU1\"].mean()\n",
    "FRUIT1_mean     = x_tr_temp[\"FRUIT1\"].mean()\n",
    "FVBEANS_mean    = x_tr_temp[\"FVBEANS\"].mean()\n",
    "FVGREEN_mean    = x_tr_temp[\"FVGREEN\"].mean()\n",
    "FVORANG_mean    = x_tr_temp[\"FVORANG\"].mean()\n",
    "VEGETAB1_mean   = x_tr_temp[\"VEGETAB1\"].mean()\n",
    "EXERHMM1_mean   = x_tr_temp[\"EXERHMM1\"].mean()\n",
    "\n",
    "#define the medians we will use to replace the NaN\n",
    "GENHLTH_median  = x_tr_temp[\"GENHLTH\"][x_tr_temp[\"GENHLTH\"].isin(np.arange(1, 6))].median()\n",
    "HLTHPLN1_median = x_tr_temp[\"HLTHPLN1\"][x_tr_temp[\"HLTHPLN1\"].isin(np.arange(1, 3))].median()\n",
    "CHECKUP1_median = x_tr_temp[\"CHECKUP1\"][x_tr_temp[\"CHECKUP1\"].isin(np.arange(0, 5))].median()\n",
    "BPMEDS_median   = x_tr_temp[\"BPMEDS\"][x_tr_temp[\"BPMEDS\"].isin(np.arange(1, 2))].median()\n",
    "TOLDHI2_median  = x_tr_temp[\"TOLDHI2\"][x_tr_temp[\"TOLDHI2\"].isin(np.arange(1, 3))].median()\n",
    "CVDSTRK3_median = x_tr_temp[\"CVDSTRK3\"][x_tr_temp[\"CVDSTRK3\"].isin(np.arange(1, 3))].median()\n",
    "CHCKIDNY_median = x_tr_temp[\"CHCKIDNY\"][x_tr_temp[\"CHCKIDNY\"].isin(np.arange(1, 3))].median()\n",
    "CHCOCNCR_median = x_tr_temp[\"CHCOCNCR\"][x_tr_temp[\"CHCOCNCR\"].isin(np.arange(1, 3))].median()\n",
    "HAVARTH3_median = x_tr_temp[\"HAVARTH3\"][x_tr_temp[\"HAVARTH3\"].isin(np.arange(1, 3))].median()\n",
    "DIABETE3_median = x_tr_temp[\"DIABETE3\"][x_tr_temp[\"DIABETE3\"].isin(np.arange(1, 5))].median()\n",
    "CHCCOPD1_median = x_tr_temp[\"CHCCOPD1\"][x_tr_temp[\"CHCCOPD1\"].isin(np.arange(1, 5))].median()\n",
    "ASTHMA3_median  = x_tr_temp[\"ASTHMA3\"][x_tr_temp[\"ASTHMA3\"].isin(np.arange(1, 3))].median()\n",
    "ASTHNOW_median  = x_tr_temp[\"ASTHNOW\"][x_tr_temp[\"ASTHNOW\"].isin(np.arange(1, 3))].median()\n",
    "CHCSCNCR_median = x_tr_temp[\"CHCSCNCR\"][x_tr_temp[\"CHCSCNCR\"].isin(np.arange(1, 3))].median()\n",
    "ADDEPEV2_median = x_tr_temp[\"ADDEPEV2\"][x_tr_temp[\"ADDEPEV2\"].isin(np.arange(1, 3))].median()\n",
    "EDUCA_median    = x_tr_temp[\"EDUCA\"][x_tr_temp[\"EDUCA\"].isin(np.arange(1, 7))].median()\n",
    "INCOME2_median  = x_tr_temp[\"INCOME2\"][x_tr_temp[\"INCOME2\"].isin(np.arange(1, 9))].median()\n",
    "QLACTLM2_median = x_tr_temp[\"QLACTLM2\"][x_tr_temp[\"QLACTLM2\"].isin(np.arange(1, 3))].median()\n",
    "USEEQUIP_median = x_tr_temp[\"USEEQUIP\"][x_tr_temp[\"USEEQUIP\"].isin(np.arange(1, 3))].median()\n",
    "BLIND_median    = x_tr_temp[\"BLIND\"][x_tr_temp[\"BLIND\"].isin(np.arange(1, 3))].median()\n",
    "DECIDE_median   = x_tr_temp[\"DECIDE\"][x_tr_temp[\"DECIDE\"].isin(np.arange(1, 3))].median()\n",
    "DIFFWALK_median = x_tr_temp[\"DIFFWALK\"][x_tr_temp[\"DIFFWALK\"].isin(np.arange(1, 3))].median()\n",
    "DIFFDRES_median = x_tr_temp[\"DIFFDRES\"][x_tr_temp[\"DIFFDRES\"].isin(np.arange(1, 3))].median()\n",
    "DIFFALON_median = x_tr_temp[\"DIFFALON\"][x_tr_temp[\"DIFFALON\"].isin(np.arange(1, 3))].median()\n",
    "SMOKE100_median = x_tr_temp[\"SMOKE100\"][x_tr_temp[\"SMOKE100\"].isin(np.arange(1, 3))].median()\n",
    "SMOKDAY2_median = x_tr_temp[\"SMOKDAY2\"][x_tr_temp[\"SMOKDAY2\"].isin(np.arange(1, 4))].median()\n",
    "USENOW3_median  = x_tr_temp[\"USENOW3\"][x_tr_temp[\"USENOW3\"].isin(np.arange(1, 4))].median()\n",
    "ALCDAY5_median  = x_tr_temp[\"ALCDAY5\"].median()\n",
    "AVEDRNK2_median = x_tr_temp[\"AVEDRNK2\"].median()\n",
    "DRNK3GE5_median = x_tr_temp[\"DRNK3GE5\"].median()\n",
    "MAXDRNKS_median = x_tr_temp[\"MAXDRNKS\"].median()\n",
    "EXERANY2_median = x_tr_temp[\"EXERANY2\"][x_tr_temp[\"EXERANY2\"].isin(np.arange(1, 3))].median()\n",
    "SEATBELT_median = x_tr_temp[\"SEATBELT\"][x_tr_temp[\"SEATBELT\"].isin(np.arange(1, 6))].median()\n",
    "PNEUVAC3_median = x_tr_temp[\"PNEUVAC3\"][x_tr_temp[\"PNEUVAC3\"].isin(np.arange(1, 3))].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#list of features to replace NaN with the mean\n",
    "mean_features   = [\"POORHLTH\", \"PHYSHLTH\", \"MENTHLTH\", \"WEIGHT2\", \"DIABAGE2\", \"WEIGHT2\", \"HEIGHT3\", \"FRUITJU1\", \"FRUIT1\", \"FVBEANS\", \"FVGREEN\", \"FVORANG\", \"VEGETAB1\",\n",
    "                   \"EXERHMM1\"]\n",
    "\n",
    "#list of features to replace NaN with the median\n",
    "median_features = [\"GENHLTH\", \"HLTHPLN1\", \"CHECKUP1\", \"BPMEDS\", \"TOLDHI2\", \"CVDSTRK3\", \"CHCKIDNY\", \"CHCOCNCR\", \"HAVARTH3\", \"DIABETE3\", \"CHCCOPD1\", \"ASTHMA3\", \"ASTHNOW\", \"CHCSCNCR\",\n",
    "                   \"ADDEPEV2\", \"EDUCA\", \"INCOME2\", \"QLACTLM2\", \"USEEQUIP\", \"BLIND\", \"DECIDE\", \"DIFFWALK\", \"DIFFDRES\", \"DIFFALON\", \"SMOKE100\", \"SMOKDAY2\", \"USENOW3\", \"ALCDAY5\", \"AVEDRNK2\",\n",
    "                   \"DRNK3GE5\", \"MAXDRNKS\", \"EXERANY2\", \"SEATBELT\", \"PNEUVAC3\"]\n",
    "\n",
    "#replace the NaN with the mean\n",
    "for feature in mean_features:\n",
    "    replace_nan(x_tr_temp, feature, eval(feature + \"_mean\"))\n",
    "\n",
    "#replace the NaN with the median\n",
    "for feature in median_features:\n",
    "    replace_nan(x_tr_temp, feature, eval(feature + \"_median\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_features=list(dico_transfos.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in interesting_features:\n",
    "    if col not in x_tr_temp.columns:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_cleaned=x_tr_temp[interesting_features].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_cleaned=y_tr.copy()\n",
    "y_tr_cleaned=y_tr_cleaned[\"_MICHD\"]\n",
    "y_tr_cleaned.replace({-1:0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(x_tr_cleaned.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135, 38)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_cleaned.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tra, x_val, y_tra, y_val=split_data(x_tr_cleaned.values,y_tr_cleaned.values.ravel(),ratio=0.75,seed=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(x,w,threshold,apply_sigmoid):\n",
    "    w2=w.ravel()\n",
    "    y_pred=x.dot(w2.T)\n",
    "    if threshold==None:\n",
    "        threshold=0.5\n",
    "    if apply_sigmoid:\n",
    "        y_pred=sigmoid(y_pred)\n",
    "    y_pred=np.array([0 if prediction<threshold else 1 for prediction in y_pred])\n",
    "    return y_pred\n",
    "def compute_scores(x,w,y,threshold=None,apply_sigmoid=False):\n",
    "    y_pred=make_predictions(x,w,threshold,apply_sigmoid) \n",
    "    TP=np.sum(np.logical_and(y_pred==1,y==1))\n",
    "    FP=np.sum(np.logical_and(y_pred==1,y==0))\n",
    "    FN=np.sum(np.logical_and(y_pred==0,y==1))\n",
    "    precision=TP/(TP+FP)\n",
    "    recall=TP/(TP+FN)\n",
    "    f1=2*precision*recall/(precision+recall)\n",
    "    return precision,recall,f1\n",
    "    \n",
    "x_tra_scaled=(x_tra-np.mean(x_tra,axis=0)[None,:])/np.std(x_tra,axis=0)\n",
    "x_val_scaled=(x_val-np.mean(x_tra,axis=0)[None,:])/np.std(x_tra,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08872373537693874"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_tra==1)[0].shape[0]/y_tra.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_ones = int(0.3 * len(y_tra))\n",
    "existing_ones_indices = np.where(y_tra == 1)[0]\n",
    "zeros_indices = np.where(y_tra == 0)[0]\n",
    "ones_indices = np.random.choice(existing_ones_indices, required_ones - len(existing_ones_indices), replace=True)\n",
    "zeros_indices=np.random.choice(zeros_indices,len(y_tra)-required_ones,replace=False)\n",
    "result_indices = np.concatenate([existing_ones_indices, ones_indices,zeros_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tra_scaled1=x_tra_scaled[result_indices].copy()\n",
    "y_tra1=y_tra[result_indices].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NN model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.333333333333332"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "38*2/3-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6307\n",
      "learning rate: 0.3\n",
      "Epoch 0, F1 Score: 0.0004\n",
      "Epoch 100, Loss: 0.7489\n",
      "learning rate: 0.23213428124999994\n",
      "Epoch 200, Loss: 0.5260\n",
      "learning rate: 0.20950118882812493\n",
      "Epoch 300, Loss: 0.6282\n",
      "learning rate: 0.1706400276829379\n",
      "Epoch 400, Loss: 0.5865\n",
      "learning rate: 0.12543610056575313\n",
      "Epoch 500, Loss: 0.5153\n",
      "learning rate: 0.09706006349211271\n",
      "Epoch 600, Loss: 0.9870\n",
      "learning rate: 0.08321687193655013\n",
      "Epoch 700, Loss: 0.4972\n",
      "learning rate: 0.06439162918288119\n",
      "Epoch 800, Loss: 0.4707\n",
      "learning rate: 0.03855364696953094\n",
      "Epoch 900, Loss: 0.4664\n",
      "learning rate: 0.03479466639000167\n"
     ]
    }
   ],
   "source": [
    "input_size = x_tra_scaled.shape[1]\n",
    "hidden_size = 25\n",
    "output_size = 1\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the model\n",
    "model.train(x_tra_scaled1, y_tra1.reshape(-1,1), epochs=1000,batching=True,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5836823061421711\n",
      "0.3810948747360896\n",
      "0.36810797300674836\n"
     ]
    }
   ],
   "source": [
    "print(model.compute_f1_score(x_tra_scaled1,y_tra1.reshape(-1,1)))\n",
    "print(model.compute_f1_score(x_tra_scaled,y_tra.reshape(-1,1)))\n",
    "print(model.compute_f1_score(x_val_scaled,y_val.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression for feature selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
